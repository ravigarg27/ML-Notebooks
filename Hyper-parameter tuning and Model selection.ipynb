{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning in Python and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "- Intro to Hyperparameter tuning\n",
    "- Intro to Hyperopt\n",
    "- Intro to pySMAC\n",
    "- Branin function benchmark\n",
    "- Parameter fitting using Grid search, Random search, Hyperopt and SMAC\n",
    "- Parameter fitting in xgboost\n",
    "- Model selection using Hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "Hyperparameter tuning in itself is also an optimization problem. We want to find a configuration of hyperparameters to maximize the model performance. However the relationship between the performance and hyperparameter can not be directly formulated as a set of mathematical equations. It is a black box. Since its a black box and we don't have mathematical equations, we don't have gradients to optimize over. Thereby we cannot use standard optimization methods of gradient decent.\n",
    "\n",
    "### There are three common methods of hyperparameter tuning: grid search, random search and automatic tuning. \n",
    "\n",
    "1. **Grid Search**\n",
    "    - In grid search, we try a set of configurations of hyperparameters and train the algorithm accordingly, finally choosing the hyperparameter configuration that gives the best performance.\n",
    "    - In practice, we specify the bounds and steps between values of the hyperparameters, so that it forms a grid of configurations.\n",
    "    - Typically we start with a limited grid with relatively large steps between parameter values, then extend or make the grid finer at the best configuration and continue searching on the new grid. \n",
    "\n",
    "2. **Random Search**\n",
    "    - In random search, we navigate the grid randomly and it has been shown that one can obtain similar performance to a full grid search. \n",
    "    - The authors in #TODO show that if the close-to-optimal region of hyperparameters occupies at least 5% of the search space, then a random search with a certain number of trials (typically 40-60 trials) will be able to find that region with high probability.\n",
    "\n",
    "3. **Automatic hyperparameter tuning**\n",
    "    - In both grid or random search, we try configurations randomly. The next trial is independent to all the trials done before. \n",
    "    - In contrast, automatic hyperparameter tuning infers knowledge about the relation between the hyperparameter settings and the corresponding model performance in order to make a better choice for the next trial. In other words, it collects the performance at several initial configurations, makes an inference and then decide what configuration to try next.\n",
    "    - This process is sequential and thereby can not be parallelized.\n",
    "    - The automatic tuning methods fall into Sequential Model based Optimization - where the black box is modeled in form of an approximate surrogate function. The configuration that maximizes the surrogate function should be tried next. \n",
    "    - SMBO methods differ in the way they formulate the surrogate function and then how they optimize the surrogate function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Automatic hyperparameter tuning methods\n",
    "1. **Bayesian Optimization**\n",
    "    - It uses Gaussian process to model the surrogate and optimizes Expected Improvement which is the expected probability that new trail will improve the current best optimization. \n",
    "    - Bayesian Optimization typically gives non-trivial, off-the-grid values for continuous hyperparameters (like the learning rate, regularization coefficient,â€¦) and was shown to beat human performance on some good benchmark datasets. \n",
    "    - A well-known implementation of Bayesian Optimization is Spearmint. \n",
    "    - However, the process is too slow. Gaussian process is a distribution over functions, so a sample from the distribution is a function. Training a Gaussian Process involves fitting this distribution to the given data, so that it generates functions that are close to the observed data. This is a very slow process.\n",
    "\n",
    "2. **Sequential Model Algorithmic Configuration (SMAC)**\n",
    "    - This uses a random forest of regression trees to model the objective function, new points are sampled from the region considered optimal (high Expected Improvement) by the random forest. The python interface is pySMAC. \n",
    "    \n",
    "3. **Tree structured parzen estimator**\n",
    "    - This is an improved version of SMAC, where two separated models are used to model the posterior. A well-known implementation of TPE is hyperopt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search vs Random search\n",
    "- Grid search is slow and costly. If we have n hyperparameters with each having 2 values, then we have a total of 2^n configurations.\n",
    "- However, grid search can be easily parallalized, where each core or worker does an independent configuration.\n",
    "- Random search is simple and effective and is thereby considered de facto method. It can also be parallelized giving results in fewer trials. \n",
    "- Grid search can search over the entire search space while Random search can miss some of area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to hyperopt - (pip install hyperopt, pip install pymongo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple example. Say we want to minimize a quadratic funtion.\n",
    "\\begin{align}\n",
    "\\dot{y} & = (x - 1)^2\n",
    "\\end{align}\n",
    "We chose the parameter space to be (-3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 0.9990134255047477}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst = fmin(fn=lambda x : (x-1)**2, space=hp.uniform('x',-3,3), algo=tpe.suggest, max_evals=1000)\n",
    "bst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To break it down the fmin function takes as arguments** \n",
    "- The function to minimize. \n",
    "- The space over which optimization takes place \n",
    "- The algorithm for optimization \n",
    "- Number of evaluations the fmin function performs (more evaluations more precise the result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter space**\n",
    "- Uniform\n",
    "- Categorical choice\n",
    "- Normal\n",
    "- qlognormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trials\n",
    "To see what is happening under the hood, we can use trials object. The trials object allows to store info at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best: {'x': -0.06454879705011476}\n",
      "trials:\n",
      "{'refresh_time': datetime.datetime(2017, 3, 7, 21, 31, 31, 171000), 'book_time': datetime.datetime(2017, 3, 7, 21, 31, 31, 170000), 'misc': {'tid': 0, 'idxs': {'x': [0]}, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'vals': {'x': [-4.346539596313321]}, 'workdir': None}, 'state': 2, 'tid': 0, 'exp_key': None, 'version': 0, 'result': {'status': 'ok', 'loss': 18.89240646231957}, 'owner': None, 'spec': None}\n",
      "{'refresh_time': datetime.datetime(2017, 3, 7, 21, 31, 31, 172000), 'book_time': datetime.datetime(2017, 3, 7, 21, 31, 31, 172000), 'misc': {'tid': 1, 'idxs': {'x': [1]}, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'vals': {'x': [-2.172642660363638]}, 'workdir': None}, 'state': 2, 'tid': 1, 'exp_key': None, 'version': 0, 'result': {'status': 'ok', 'loss': 4.720376129631987}, 'owner': None, 'spec': None}\n"
     ]
    }
   ],
   "source": [
    "fspace = {\n",
    "    'x': hp.uniform('x', -5, 5)\n",
    "}\n",
    "\n",
    "def f(params):\n",
    "    x = params['x']\n",
    "    val = x**2\n",
    "    return {'loss': val, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=f, space=fspace, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "\n",
    "print 'best:', best\n",
    "\n",
    "print 'trials:'\n",
    "for trial in trials.trials[:2]:\n",
    "    print trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to pySMAC (pip install pysmac) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Branin function\n",
    "\n",
    "Let's take a simple function to understand the mechanism how each of the method works. We will benchmark each of the method using time and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def branin(params):\n",
    "    \n",
    "    x0 = params['x0']\n",
    "    x1 = params['x1']\n",
    "    b = (5.1 / (4.*np.pi**2))\n",
    "    c = (5. / np.pi)\n",
    "    t = (1. / (8.*np.pi))\n",
    "    score = 1.*(x1-b*x0**2+c*x0-6.)**2+10.*(1-t)*np.cos(x0)+10\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "space = {\n",
    "    'x0' : hp.uniform('x0', -5, 10),\n",
    "    'x1' : hp.uniform('x1', 0, 15)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x0': -3.121364434058067, 'x1': 12.159966735369272}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst = fmin(branin, space, tpe.suggest, 1000)\n",
    "bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Machine learning/ Scikit learn classifier example \n",
    "\n",
    "- We will use the iris dataset here for analysis and tune the parameters for Random Forrest classifier. However, I provide free data loader script to use for any dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Free loader script. We don't use it here\n",
    "def load_train(traindata, labelCol=None, idCol=None):\n",
    "    train = pd.read_csv(traindata)\n",
    "    if labelCol:\n",
    "        labels = train[labelCol].values\n",
    "        lbl_enc = preprocessing.LabelEncoder()\n",
    "        labels = lbl_enc.fit_transform(labels)\n",
    "        train = train.drop(labelCol, axis=1)\n",
    "    if idCol:\n",
    "        train = train.drop(idCol, axis=1)\n",
    "    \n",
    "    return train.values, labels.astype('int32')\n",
    "\n",
    "def load_test(atestdata, idCol=None):\n",
    "    test = pd.read_csv(testdata)\n",
    "    if idCol:\n",
    "        test = test.drop(idCol, axis=1)\n",
    "    return test.values\n",
    "\n",
    "def encode_labels(labels):\n",
    "    lbl_enc = preprocessing.LabelEncoder()\n",
    "    labels = lbl_enc.fit_transform(labels)\n",
    "    \n",
    "    return lbl_enc, lables.astype(int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150L, 4L)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150L,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify a classifier and a cross validation scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#specify a classifier\n",
    "clf = RandomForestClassifier(n_estimators=20)\n",
    "\n",
    "#specify a cross-validation scheme\n",
    "cv = StratifiedKFold(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# specify parameters space\n",
    "param_dist = {\"max_depth\": [1,2,3,4],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"max_features\": [1,2,3,4],\n",
    "              \"min_samples_split\": [2,3,4,5],\n",
    "              \"min_samples_leaf\": [1,2,3,4],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# space for hyperopt\n",
    "space4rf = {\n",
    "    'max_depth': hp.choice('max_depth', range(1,5)),\n",
    "    'max_features': hp.choice('max_features', range(1,5)),\n",
    "    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"]),\n",
    "    'bootstrap' : hp.choice('bootstrap', [True, False]),\n",
    "    \"min_samples_split\": hp.choice('min_samples_split', range(2,6)),\n",
    "    \"min_samples_leaf\": hp.choice('min_samples_leaf', range(1,5))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 252.48 seconds for 1024 candidate parameter settings.\n"
     ]
    }
   ],
   "source": [
    "rfGridCV = GridSearchCV(clf, param_grid=param_dist, cv=cv, n_jobs=-1)\n",
    "start = time()\n",
    "rfGridCV.fit(X, y)\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(rfGridCV.cv_results_['params'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 5.79 seconds for 20 candidates parameter settings.\n"
     ]
    }
   ],
   "source": [
    "n_iter_search = 20\n",
    "rfRandomCV = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, cv=cv, n_jobs=-1)\n",
    "\n",
    "start = time()\n",
    "rfRandomCV.fit(X, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperopt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76 {'max_features': 2, 'criterion': 'entropy', 'min_samples_split': 2, 'bootstrap': True, 'max_depth': 1, 'min_samples_leaf': 1}\n",
      "0.946666666667 {'max_features': 3, 'criterion': 'entropy', 'min_samples_split': 5, 'bootstrap': True, 'max_depth': 4, 'min_samples_leaf': 3}\n",
      "0.96 {'max_features': 3, 'criterion': 'gini', 'min_samples_split': 4, 'bootstrap': True, 'max_depth': 3, 'min_samples_leaf': 2}\n",
      "0.966666666667 {'max_features': 2, 'criterion': 'entropy', 'min_samples_split': 5, 'bootstrap': True, 'max_depth': 4, 'min_samples_leaf': 3}\n",
      "Hyperopt took 25.34 seconds for 20 candidates parameter settings.\n"
     ]
    }
   ],
   "source": [
    "best = 0.0\n",
    "def objective(params):\n",
    "    global best\n",
    "    clf = RandomForestClassifier(**params)\n",
    "    acc = cross_val_score(clf, X, y, cv=cv, n_jobs=-1).mean()\n",
    "    if acc > best:\n",
    "        best = acc\n",
    "        print best, params\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "start = time()\n",
    "hyperoptBest = fmin(objective, space4rf, algo=tpe.suggest, max_evals=20, trials=trials)\n",
    "print(\"Hyperopt took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pySMAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the best score and the parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.96666666666666667,\n",
       " {'bootstrap': True,\n",
       "  'criterion': 'gini',\n",
       "  'max_depth': 1,\n",
       "  'max_features': 3,\n",
       "  'min_samples_leaf': 2,\n",
       "  'min_samples_split': 2})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfGridCV.best_score_, rfGridCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.96666666666666667,\n",
       " {'bootstrap': True,\n",
       "  'criterion': 'entropy',\n",
       "  'max_depth': 3,\n",
       "  'max_features': 2,\n",
       "  'min_samples_leaf': 2,\n",
       "  'min_samples_split': 5})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfRandomCV.best_score_, rfRandomCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': 0,\n",
       " 'criterion': 1,\n",
       " 'max_depth': 3,\n",
       " 'max_features': 1,\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 3}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperoptBest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X, y)\n",
    "\n",
    "def scoreXGB(params):\n",
    "    print \"Training with params : \"\n",
    "    print params\n",
    "    num_round = int(params['n_estimators'])\n",
    "    del params['n_estimators']\n",
    "    \n",
    "    # watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "    model = xgb.train(params, dtrain, num_round)\n",
    "    predictions = model.predict(dvalid).reshape((X_test.shape[0], 9))\n",
    "    score = log_loss(y_test, predictions)\n",
    "    print \"\\tScore {0}\\n\\n\".format(score)\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "spaceXGB = {\n",
    "             'n_estimators' : hp.quniform('n_estimators', 100, 1000, 1),\n",
    "             'eta' : hp.quniform('eta', 0.025, 0.5, 0.025),\n",
    "             'max_depth' : hp.quniform('max_depth', 1, 13, 1),\n",
    "             'min_child_weight' : hp.quniform('min_child_weight', 1, 6, 1),\n",
    "             'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "             'gamma' : hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "             'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "             'num_class' : 9,\n",
    "             'eval_metric': 'mlogloss',\n",
    "             'objective': 'multi:softprob',\n",
    "             'nthread' : 6,\n",
    "             'silent' : 1\n",
    "             }\n",
    "\n",
    "trials = Trials()\n",
    "start = time()\n",
    "hyperoptBest = fmin(scoreXGB, spaceXGB, algo=tpe.suggest, max_evals=20, trials=trials)\n",
    "print(\"XGBoost - Hyperopt took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection using hyperopt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best: 0.333333333333 using naive_bayes\n",
      "new best: 0.927287581699 using knn\n",
      "new best: 0.953839869281 using svm\n",
      "new best: 0.979983660131 using svm\n",
      "new best: 0.986928104575 using svm\n",
      "new best: 0.993464052288 using svm\n",
      "iters: 50 , acc: 0 using {'normalize': 1, 'n_estimators': 18, 'scale': 0, 'criterion': 'entropy', 'max_features': 4, 'type': 'randomforest', 'max_depth': 13}\n",
      "iters: 100 , acc: 0.979983660131 using {'kernel': 'linear', 'C': 0.8262032215744757, 'type': 'svm', 'gamma': 12.176832973486928}\n",
      "iters: 150 , acc: 0.979983660131 using {'kernel': 'linear', 'C': 0.5254848537239812, 'type': 'svm', 'gamma': 14.809575965678553}\n",
      "iters: 200 , acc: 0.986928104575 using {'kernel': 'linear', 'C': 2.0015050195918827, 'type': 'svm', 'gamma': 10.103045550524602}\n",
      "iters: 250 , acc: 0.333333333333 using {'alpha': 1.2427011171500757, 'type': 'naive_bayes'}\n",
      "iters: 300 , acc: 0 using {'normalize': 0, 'n_estimators': 3, 'scale': 0, 'criterion': 'gini', 'max_features': 4, 'type': 'randomforest', 'max_depth': 3}\n",
      "iters: 350 , acc: 0.967320261438 using {'kernel': 'linear', 'C': 3.3153686512112834, 'type': 'svm', 'gamma': 16.59722237239461}\n",
      "iters: 400 , acc: 0.986928104575 using {'kernel': 'linear', 'C': 2.034484121092214, 'type': 'svm', 'gamma': 10.616689447260757}\n",
      "iters: 450 , acc: 0.986928104575 using {'kernel': 'linear', 'C': 1.9960915549592868, 'type': 'svm', 'gamma': 19.446282087660414}\n",
      "iters: 500 , acc: 0.736519607843 using {'kernel': 'rbf', 'C': 0.062779714958342, 'type': 'svm', 'gamma': 13.01751922951427}\n",
      "iters: 550 , acc: 0.333333333333 using {'alpha': 0.5576677370628211, 'type': 'naive_bayes'}\n",
      "iters: 600 , acc: 0.986928104575 using {'kernel': 'linear', 'C': 2.065373342122141, 'type': 'svm', 'gamma': 17.267650454444066}\n",
      "iters: 650 , acc: 0.967320261438 using {'kernel': 'linear', 'C': 2.8799780044618393, 'type': 'svm', 'gamma': 17.733086139681852}\n",
      "iters: 700 , acc: 0.993464052288 using {'kernel': 'linear', 'C': 1.8645523398517643, 'type': 'svm', 'gamma': 14.673744291289022}\n",
      "iters: 750 , acc: 0.986928104575 using {'kernel': 'linear', 'C': 1.5803052515325677, 'type': 'svm', 'gamma': 12.243070059968161}\n",
      "iters: 800 , acc: 0.986928104575 using {'kernel': 'linear', 'C': 1.6380615172466486, 'type': 'svm', 'gamma': 16.215812524506937}\n",
      "iters: 850 , acc: 0.97385620915 using {'kernel': 'linear', 'C': 2.464245323122516, 'type': 'svm', 'gamma': 15.108059129605293}\n",
      "iters: 900 , acc: 0 using {'normalize': 0, 'n_estimators': 18, 'scale': 0, 'criterion': 'gini', 'max_features': 4, 'type': 'randomforest', 'max_depth': 8}\n",
      "iters: 950 , acc: 0 using {'normalize': 0, 'n_estimators': 6, 'scale': 0, 'criterion': 'gini', 'max_features': 4, 'type': 'randomforest', 'max_depth': 11}\n",
      "iters: 1000 , acc: 0.906862745098 using {'n_neighbors': 42, 'type': 'knn'}\n",
      "iters: 1050 , acc: 0.986519607843 using {'kernel': 'linear', 'C': 0.7180173301341017, 'type': 'svm', 'gamma': 11.038218793097059}\n",
      "iters: 1100 , acc: 0.986928104575 using {'kernel': 'linear', 'C': 1.8836611277807047, 'type': 'svm', 'gamma': 14.599145015743035}\n",
      "iters: 1150 , acc: 0.967320261438 using {'kernel': 'linear', 'C': 3.382981448291683, 'type': 'svm', 'gamma': 10.450049363364231}\n",
      "iters: 1200 , acc: 0.986928104575 using {'kernel': 'linear', 'C': 1.7109271314523393, 'type': 'svm', 'gamma': 2.190117521286837}\n",
      "iters: 1250 , acc: 0.967320261438 using {'n_neighbors': 15, 'type': 'knn'}\n",
      "iters: 1300 , acc: 0.986928104575 using {'kernel': 'linear', 'C': 1.1626381430358705, 'type': 'svm', 'gamma': 16.864886408942027}\n",
      "iters: 1350 , acc: 0.993464052288 using {'kernel': 'linear', 'C': 1.4141084112042017, 'type': 'svm', 'gamma': 14.417110859488544}\n",
      "iters: 1400 , acc: 0.986928104575 using {'kernel': 'linear', 'C': 1.6015617641845274, 'type': 'svm', 'gamma': 13.862465786730485}\n",
      "iters: 1450 , acc: 0.934232026144 using {'n_neighbors': 34, 'type': 'knn'}\n",
      "iters: 1500 , acc: 0.986519607843 using {'kernel': 'linear', 'C': 0.5815208793426004, 'type': 'svm', 'gamma': 12.296977094127968}\n",
      "best:\n",
      "{'kernel': 0, 'C': 1.4022234064239614, 'classifier_type': 1, 'gamma': 16.780507968298693}\n"
     ]
    }
   ],
   "source": [
    "def hyperopt_train_test(params):\n",
    "    t = params['type']\n",
    "    del params['type']\n",
    "    if t == 'naive_bayes':\n",
    "        clf = BernoulliNB(**params)\n",
    "    elif t == 'svm':\n",
    "        clf = SVC(**params)\n",
    "    elif t == 'dtree':\n",
    "        clf = DecisionTreeClassifier(**params)\n",
    "    elif t == 'knn':\n",
    "        clf = KNeighborsClassifier(**params)\n",
    "    else:\n",
    "        return 0\n",
    "    return cross_val_score(clf, X, y).mean()\n",
    "\n",
    "space = hp.choice('classifier_type', [\n",
    "    {\n",
    "        'type': 'naive_bayes',\n",
    "        'alpha': hp.uniform('alpha', 0.0, 2.0)\n",
    "    },\n",
    "    {\n",
    "        'type': 'svm',\n",
    "        'C': hp.uniform('C', 0, 10.0),\n",
    "        'kernel': hp.choice('kernel', ['linear', 'rbf']),\n",
    "        'gamma': hp.uniform('gamma', 0, 20.0)\n",
    "    },\n",
    "    {\n",
    "        'type': 'randomforest',\n",
    "        'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "        'max_features': hp.choice('max_features', range(1,5)),\n",
    "        'n_estimators': hp.choice('n_estimators', range(1,20)),\n",
    "        'criterion': hp.choice('criterion', [\"gini\", \"entropy\"]),\n",
    "        'scale': hp.choice('scale', [0, 1]),\n",
    "        'normalize': hp.choice('normalize', [0, 1])\n",
    "    },\n",
    "    {\n",
    "        'type': 'knn',\n",
    "        'n_neighbors': hp.choice('knn_n_neighbors', range(1,50))\n",
    "    }\n",
    "])\n",
    "\n",
    "count = 0\n",
    "best = 0\n",
    "def f(params):\n",
    "    global best, count\n",
    "    count += 1\n",
    "    acc = hyperopt_train_test(params.copy())\n",
    "    if acc > best:\n",
    "        print 'new best:', acc, 'using', params['type']\n",
    "        best = acc\n",
    "    if count % 50 == 0:\n",
    "        print 'iters:', count, ', acc:', acc, 'using', params\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(f, space, algo=tpe.suggest, max_evals=1500, trials=trials)\n",
    "print 'best:'\n",
    "print best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
